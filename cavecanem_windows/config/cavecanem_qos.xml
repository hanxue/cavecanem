<dds>
  <qos_library name="cavecanem_publisher">
    <qos_profile name="Reliable">
      <datareader_qos>
        <reliability>
          <kind>RELIABLE_RELIABILITY_QOS</kind>
        </reliability>
        <history>
          <kind>KEEP_ALL_HISTORY_QOS</kind>
        </history>
        <protocol>
          <rtps_reliable_reader>
            <min_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_heartbeat_response_delay>
            <max_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_heartbeat_response_delay>
          </rtps_reliable_reader>
        </protocol>
      </datareader_qos>
      
      <datawriter_qos>      
        <reliability>
          <kind>RELIABLE_RELIABILITY_QOS</kind>
          <max_blocking_time>
            <sec>5</sec>
            <nanosec>0</nanosec>
          </max_blocking_time>
        </reliability>
        <history>
          <kind>KEEP_ALL_HISTORY_QOS</kind>
        </history>
        <resource_limits>
          <max_samples>32</max_samples>
        </resource_limits>
        <protocol>
          <rtps_reliable_writer>
            <low_watermark>5</low_watermark>
            <high_watermark>15</high_watermark>
            <heartbeat_period>
              <sec>0</sec>
              <nanosec>100000000</nanosec>
            </heartbeat_period>
            <fast_heartbeat_period>
              <sec>0</sec>
              <nanosec>10000000</nanosec>
            </fast_heartbeat_period>
            <late_joiner_heartbeat_period>
              <sec>0</sec>
              <nanosec>10000000</nanosec>
            </late_joiner_heartbeat_period>
            <max_heartbeat_retries>500</max_heartbeat_retries>
            <min_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_nack_response_delay>
            <max_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_nack_response_delay>
            <min_send_window_size>32</min_send_window_size>
            <max_send_window_size>32</max_send_window_size>
          </rtps_reliable_writer>
        </protocol>
      </datawriter_qos>
    </qos_profile>
    
    <!--
       The LossyNetwork profile extends the Reliable profile to perform
       additional, finer-grainer performance tuning specific to applications
       that send continuously streaming data. The parameters specified here
       add to and/or override the parameters specified in the Reliable
       profile.
      -->
    <qos_profile name="lossy_network"
                 base_name="Reliable">
      <datawriter_qos>
        <reliability>
          <!--
             Set the maximum writer blocking time to a relatively large
             value because the link is assumed to have high latency.
             If the link is assumed to have a worst-case round-trip
             latency of 1 second, 10 seconds is a reasonable
             starting point.
            -->
          <max_blocking_time>
            <sec>10</sec>
            <nanosec>0</nanosec>
          </max_blocking_time>
        </reliability>
	
        <!--
           The writer cache size is chosen not to exceed the bandwidth
           limitation in case the writer needs resend the entire cache
           of samples because of packet loss. If the worst-case round-
           trip latency is 1 s, and the throughput is 256 Kb/s, and
           we're sending data in 1400 B chunks, then we can't tolerate
           a cache much larger than 20 samples.
          -->
        <resource_limits>
          <max_samples>20</max_samples>
          <initial_samples>20</initial_samples>
        </resource_limits>
        <writer_resource_limits>
          <!--
             When batching is enabled, this parameter - and not
             max_samples - is used to configure the piggyback
             heartbeat behavior.
            -->
          <max_batches>20</max_batches>
        </writer_resource_limits>

        <protocol>
          <rtps_reliable_writer>
            <!--
               Governs how often heartbeats are "piggybacked" on
               data samples. We want to send a heartbeat with every
               sample.
              -->
            <heartbeats_per_max_samples>
              20
            </heartbeats_per_max_samples>

            <!--
               There's no point in sending heartbeats faster than
               once per round-trip latency; we'd just be wasting
               bandwidth.

               At the same time, we don't want to depend entirely on
               piggybacked heartbeats, because if the writer blocks
               and its entire cache contents (or their ACKs) were
               lost, we don't want to lock up the writer.

               Heartbeating once every 1-2 seconds means we won't
               spend much bandwidth on heartbeats, but at the same
               time, we'll have several chance to get data through
               before the max_blocking_time expires.

               See reliable.xml for more information about these
               parameters.
              -->
            <heartbeat_period>
              <sec>2</sec>
              <nanosec>0</nanosec>
            </heartbeat_period>
            <fast_heartbeat_period>
              <sec>1</sec>
              <nanosec>0</nanosec>
            </fast_heartbeat_period>
            <late_joiner_heartbeat_period>
              <sec>1</sec>
              <nanosec>0</nanosec>
            </late_joiner_heartbeat_period>

            <!--
               The delay that's of concern here is the network
               latency. If there's substantial data loss, or the
               link temporarily goes down, you don't want to
               erroneously deactivate the reader, because the
               writer's cache is likely almost full almost all the
               time: if the reader comes back, it will lose a whole
               cache's-worth of samples or more.

               You could set max_heartbeat_retries to
               LENGTH_UNLIMITED to prevent the reader from ever
               being deactivated, but then you're throwing away a
               fault tolerance tool that can be very valuable if the
               reader really did go away.

               If we're sending periodic heartbeats once every
               second or two, a max_heartbeat_retries setting of 20
               leaves enough time for a write() to time out, be
               reattempted, and time out again. If the reader still
               isn't back, we'll stop waiting around for it.
              -->
            <max_heartbeat_retries>20</max_heartbeat_retries>

            <!--
               Limit the amount of repair traffic to make sure we do
               not exceed the bandwidth limitations. In this case,
               we're assuming 256 Kb/s == 32 KB/s, so a limit of
               ~28 KB gives us some slack for protocol overhead and
               the like. 
              -->
            <max_bytes_per_nack_response>
              28000
            </max_bytes_per_nack_response>

            <!--
               Set the maximum number of unacknowedged samples 
               (batches) in the DataWriter's queue equal to the max 
               number of batches.  
              -->
            <min_send_window_size>10</min_send_window_size>
            <max_send_window_size>20</max_send_window_size>

          </rtps_reliable_writer>
        </protocol>

        <!--
           We want to fill the network as efficiently as possible, while
           at the same time not exceeding the underlying transport MTU.
           We assume here an MTU of 1500 B (the same as ethernet). We
           leave some slack for protocol overhead, leaving 1400 B for
           data. If the data size is larger than than, we will need to
           fragment (see the participant transport configuration below)
           and flow control it (see 'publish_mode'). If the data size is
           smaller than that, we batch several samples together until we
           reach that limit (see 'batch').

           Fragmentation and batching cannot be used together; choose
           one or the other depending on your data size. Flow control
           is only necessary when fragmenting data, so it has been
           commented out here.
          -->
        <!--
           <publish_mode>
             <kind>ASYNCHRONOUS_PUBLISH_MODE_QOS</kind>
             <flow_controller_name>DDS_DEFAULT_FLOW_CONTROLLER_NAME</flow_controller_name>
           </publish_mode>
           -->

        <batch>
          <enable>true</enable>

          <!--
             There is some overhead in the batch in addition to the
             sample data, so to keep the total "data" size in the batch
             under the transport's maximum message size, leave some
             additional slack.
            -->
          <max_data_bytes>1300</max_data_bytes>
          <max_samples>LENGTH_UNLIMITED</max_samples>

          <!--
             If a one-way latency-worth of time has passed, we should
             have flushed the batch by now. Since the example writes
             continuously, though, we probably won't ever get to this
             point.
            -->
          <max_flush_delay>
            <sec>0</sec>
            <nanosec>500000000</nanosec><!-- 500 ms -->
          </max_flush_delay>

          <source_timestamp_resolution>
            <sec>DURATION_INFINITE_SEC</sec>
            <nanosec>DURATION_INFINITE_NSEC</nanosec>
          </source_timestamp_resolution>
        </batch>        
      </datawriter_qos>

      <participant_qos>
        <!--
           The participant name, if it is set, will be displayed in the
           RTI Analyzer tool, making it easier for you to tell one
           application from another when you're debugging.
          -->
        <participant_name>
          <name>RTI Example (Lossy Network)</name>
        </participant_name>

        <property>
          <value>
            <!--
               Configure UDPv4 transport:
               Limit the size of a datagram to the MTU of the
               underlying transport. For Ethernet, this value is
               1500 bytes; other transports will have different
               values. Then we leave some slack for UDP/IP overhead.

               Data batching and fragmentation are not currently
               supported together. For small data sizes - less than
               the MTU - leave this property commented. For large
               data sizes, comment the batching policy above and
               uncomment this property.
              -->
            <!--
               <element>
                 <name>dds.transport.UDPv4.builtin.parent.message_size_max</name>
                 <value>1400</value>
               </element>
               -->
          </value>
        </property> 
      </participant_qos>
    </qos_profile>

    <qos_profile name="reliable_base">
      <datareader_qos>
        <reliability>
          <kind>RELIABLE_RELIABILITY_QOS</kind>
        </reliability>
        <history>
          <kind>KEEP_ALL_HISTORY_QOS</kind>
        </history>
        <protocol>
          <rtps_reliable_reader>
            <min_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_heartbeat_response_delay>
            <max_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_heartbeat_response_delay>
          </rtps_reliable_reader>
        </protocol>
      </datareader_qos>
      
      <datawriter_qos>      
        <reliability>
          <kind>RELIABLE_RELIABILITY_QOS</kind>
          <max_blocking_time>
            <sec>5</sec>
            <nanosec>0</nanosec>
          </max_blocking_time>
        </reliability>
        <history>
          <kind>KEEP_ALL_HISTORY_QOS</kind>
        </history>
        <resource_limits>
          <max_samples>32</max_samples>
        </resource_limits>
        <protocol>
          <rtps_reliable_writer>
            <low_watermark>5</low_watermark>
            <high_watermark>15</high_watermark>
            <heartbeat_period>
              <sec>0</sec>
              <nanosec>100000000</nanosec>
            </heartbeat_period>
            <fast_heartbeat_period>
              <sec>0</sec>
              <nanosec>10000000</nanosec>
            </fast_heartbeat_period>
            <late_joiner_heartbeat_period>
              <sec>0</sec>
              <nanosec>10000000</nanosec>
            </late_joiner_heartbeat_period>
            <max_heartbeat_retries>500</max_heartbeat_retries>
            <min_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_nack_response_delay>
            <max_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_nack_response_delay>
            <min_send_window_size>32</min_send_window_size>
            <max_send_window_size>32</max_send_window_size>
          </rtps_reliable_writer>
        </protocol>
      </datawriter_qos>
    </qos_profile>

    <!--
       The HighThroughput profile extends the Reliable profile to perform
       additional, finer-grainer performance tuning specific to applications
       that send continuously streaming data. The parameters specified here
       add to and/or override the parameters specified in the Reliable
       profile.
      -->
    
    <qos_profile name="high_throughput">
      <datawriter_qos>
        <writer_resource_limits>
          <!--
             The number of batches (not samples) for which the
             DataWriter will allocate space.

             The initial_batches parameter is also set here, indicating
             to the writer that it should pre-allocate all of the space
             up front. Pre-allocation will remove memory allocattion
             from the critical path of the application, improving
             performance and determinism.

             Finite resources are not required for strict reliability.
             However, by limiting how far "ahead" of its readers a
             writer is able to get, you can make the system more
             robust and performant in the face of slow readers and/or
             dropped packets while at the same time constraining your
             memory growth.
            -->
          <max_batches>32</max_batches>
          <initial_batches>32</initial_batches>
        </writer_resource_limits>

        <!--
           We're limiting resources based on the number of batches. We
           could limit resources on a per-sample basis too if we wanted
           to; we'd probably to set the value based on how many samples
           we expect to be in each batch. Rather than come up with a
           heuristic, however, it's more straightforward to override
           this value to leave the value unlimited. (If you were to set
           both, the first limit to be hit would take effect.)
          -->
        <resource_limits>
          <max_samples>LENGTH_UNLIMITED</max_samples>
        </resource_limits>

        <protocol>
          <rtps_reliable_writer>
            <!--
               Speed up the heartbeat rate. See reliable.xml for
               more information about this parameter.
              -->            
            <heartbeat_period>
              <!-- 10 milliseconds: -->
              <sec>0</sec>
              <nanosec>10000000</nanosec>
            </heartbeat_period>
            <!--
               Speed up the heartbeat rate. See reliable.xml for
               more information about this parameter.
              -->
            <fast_heartbeat_period>
              <!-- 1 millisecond: -->
              <sec>0</sec>
              <nanosec>1000000</nanosec>
            </fast_heartbeat_period>
            <!--
               Speed up the heartbeat rate. See reliable.xml for
               more information about this parameter.
              -->
            <late_joiner_heartbeat_period>
              <!-- 1 millisecond: -->
              <sec>0</sec>
              <nanosec>1000000</nanosec>
            </late_joiner_heartbeat_period>
            
            <!--
               The heartbeat rate is faster, so allow more time for
               readers to respond before they are deactivated. See
               reliable.xml for more information about this parameter.
              -->
            <max_heartbeat_retries>1000</max_heartbeat_retries>

            <!--
               Set the maximum number of unacknowedged samples 
               (batches) in the DataWriter's queue equal to the max 
               number of batches.  
              -->
            <min_send_window_size>32</min_send_window_size>
            <max_send_window_size>32</max_send_window_size>

          </rtps_reliable_writer>
        </protocol>
        
        <!--
           When sending very many small data samples, the efficiency of
           the network can be increased by batching multiple samples
           together in a single protocol-level message (usually
           corresponding to a single network datagram). Batching can
           offer very substantial throughput gains, but often at the
           expense of latency, although in some configurations, the
           latency penalty can be very small or even zero - even
           negative.
          -->
        <batch>
          <enable>true</enable>
          
          <!--
             Batches can be "flushed" to the network based on a
             maximum size. This size can be based on the total number
             of bytes in the accumulated data samples and/or the number
             of samples. Whenever the first of these limits is reached,
             the batch will be flushed.
            -->
          <max_data_bytes>30720</max_data_bytes><!-- 30 KB -->
          <max_samples>LENGTH_UNLIMITED</max_samples>
          
          <!--
             Batches can be flushed to the network based on an elapsed
             time.
            -->
          <max_flush_delay>
            <sec>DURATION_INFINITE_SEC</sec>
            <nanosec>DURATION_INFINITE_NSEC</nanosec>
          </max_flush_delay>
          
          <!--
             The middleware will associate a source timestamp with a
             batch when it is started. The duration below indicates
             the amount of time that may pass before the middleware
             will insert an additional timestamp into the middle of an
             existing batch.
             
             Shortening this duration can give readers increased
             timestamp resolution if they require that. However,
             lengthening this duration decreases the amount of
             meta-data on the network, potentially improving
             throughput, especially if the data samples are very small.
             If this delay is set to an infinite time period,
             timestamps will be inserted only once per batch, and
             furthermore the middleware will not need to check the
             time with each sample in the batch, reducing the amount
             of computation on the send path and potentially improving
             both latency and throughput performance.
            -->
          <source_timestamp_resolution>
            <sec>DURATION_INFINITE_SEC</sec>
            <nanosec>DURATION_INFINITE_NSEC</nanosec>
          </source_timestamp_resolution>
        </batch>        
      </datawriter_qos>

      <participant_qos>
        <!--
           The participant name, if it is set, will be displayed in the
           RTI Analyzer tool, making it easier for you to tell one
           application from another when you're debugging.
          -->
        <participant_name>
          <name>RTI Example (High Throughput)</name>
        </participant_name>

        <receiver_pool>
          <!--
             The maximum size of a datagram that can be deserialized,
             independent of the network transport. By default, this
             value is 9 KB, since that is a common default maximum
             size for UDP datagrams on some platforms. However, on
             platforms that support larger datagrams - up to 64 KB -
             it's a good idea to increase this limit for demanding
             applications to avoid socket read errors.
            -->
          <buffer_size>65536</buffer_size><!-- 64 KB -->
        </receiver_pool>
        
        <property>
          <value>
            <!--
               Configure UDPv4 transport:
              -->
            <element>
              <!--
                 On platforms that support it, increase the maximum
                 size of a UDP datagram to the maximum supported by
                 the protocol: 64 KB. That will allow you to send
                 the large packets that can result when you batch
                 samples.
                -->
              <name>dds.transport.UDPv4.builtin.parent.message_size_max</name>
              <value>65536</value><!-- 64 KB -->
            </element>
            <element>
              <!--
                 If possible, increase the UDP send socket buffer
                 size. This will allow you to send multiple large
                 packets without UDP send errors.

                 On some platforms (e.g. Linux), this value is
                 limited by a system-wide policy. Setting it to
                 a larger value will fail silently; the value will
                 be set to the maximum allowed by that policy.
                -->
              <name>dds.transport.UDPv4.builtin.send_socket_buffer_size</name>
              <value>524288</value><!-- 512 KB -->
            </element>
            <element>
              <!--
                 If possible, increase the UDP receive socket
                 buffer size. This will allow you to receive
                 multiple large packets without UDP receive errors.

                 On some platforms (e.g. Linux), this value is
                 limited by a system-wide policy. Setting it to
                 a larger value will fail silently; the value will
                 be set to the maximum allowed by that policy.
                -->
              <name>dds.transport.UDPv4.builtin.recv_socket_buffer_size</name>
              <value>2097152</value><!-- 2 MB -->
            </element>

            <!--
               Configure shared memory transport:
              -->
            <element>
              <!--
                 Set the shared memory maximum message size to the
                 same value that was set for UDP.
                -->
              <name>dds.transport.shmem.builtin.parent.message_size_max</name>
              <value>65536</value><!-- 64 KB -->
            </element>
            <element>
              <!--
                 Set the size of the shared memory transport's
                 receive buffer to some large value.
                -->
              <name>dds.transport.shmem.builtin.receive_buffer_size</name>
              <value>2097152</value><!-- 2 MB -->
            </element>
            <element>
              <!--
                 Set the maximum number of messages that the shared
                 memory transport can cache while waiting for them
                 to be read and deserialized.
                -->
              <name>dds.transport.shmem.builtin.received_message_count_max</name>
              <value>2048</value>
            </element>

            <!--
               Increase the size of the string built-in size. This
               configuration is only necessary for applications that
               use the built-in types (such as Hello_builtin).
              -->
            <element>
              <name>dds.builtin_type.string.max_size</name>
              <value>2048</value>
            </element>
          </value>
        </property> 
      </participant_qos>
    </qos_profile>
    
    <qos_profile name="reliable">
      <datareader_qos>
        <reliability>
          <!--
             Enable reliability.
            -->
          <kind>RELIABLE_RELIABILITY_QOS</kind>
        </reliability>
        
        <history>
          <!--
             To implement strict reliability, we need to set the
             history to KEEP_ALL. That way, undelivered samples
             will not be overwritten.
            -->
          <kind>KEEP_ALL_HISTORY_QOS</kind>
        </history>

        <!--
           The following parameters tune the behavior of the reliability
           protocol. Setting them is not required in order to achieve
           strict reliability but is beneficial from a performance
           standpoint. 
          -->
        <protocol>
          <rtps_reliable_reader>
            <!--
               When the DataReader receives a heartbeat from a
               DataWriter (indicating (a) that the DataWriter still
               exists on the network and (b) what sequence numbers
               it has published), the following parameters indicate
               how long it will wait before replying with a positive
               (assuming they aren't disabled) or negative
               acknowledgement.

               The time the reader waits will be a random duration
               in between the minimum and maximum values. Narrowing
               this range, and shifting it towards zero, will make
               the system more reactive. However, it will increase
               the chance of (N)ACK spikes. The higher the number of
               readers on the topic, and the greater the load on
               your network, the more you should consider specifying
               a range here.
              -->
            <min_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </min_heartbeat_response_delay>
            <max_heartbeat_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_heartbeat_response_delay>
          </rtps_reliable_reader>
        </protocol>        
      </datareader_qos>
      
      <datawriter_qos>      
        <reliability>
          <!--
             Enable reliability.
             The writer maximum blocking time is 5 seconds.
            -->
          <kind>RELIABLE_RELIABILITY_QOS</kind>
          <max_blocking_time>
            <sec>5</sec>
            <nanosec>0</nanosec>
          </max_blocking_time>
        </reliability>

        <history>
          <!--
             To implement strict reliability, we need to set the
             history to KEEP_ALL. That way, unacknowledged samples
             will not be overwritten.
            -->
          <kind>KEEP_ALL_HISTORY_QOS</kind>
        </history>

        <resource_limits>
          <!--
             The number of data samples for which the DataWriter will
             allocate space. This example is configured without
             durability, meaning that when all DataReaders have
             acknowledged a sample, the DataWriter will discard it.
             The value below, then, effectively indicates how far
             ahead of the slowest reader the writer is able to get
             before it will block waiting for the reader(s) to catch
             up.

             Finite resources are not required for strict reliability.
             However, by limiting how far "ahead" of its readers a
             writer is able to get, you can make the system more
             robust and performant in the face of slow readers and/or
             dropped packets while at the same time constraining your
             memory growth.  This can be done by either setting 
             max_samples to be finite or setting 
             protocol.rtps_reliable_writer.[min|max]_send_window_size.
            -->
          <max_samples>32</max_samples>
        </resource_limits>

        <!--
           The following parameters tune the behavior of the reliability
           protocol. Setting them is not required in order to achieve
           strict reliability but is beneficial from a performance
           standpoint. 
          -->
        <protocol>
          <rtps_reliable_writer>
            <!--
               When the writer's cache gets down to this number of
               samples, it will slow the rate at which it sends
               heartbeats to readers.
              -->
            <low_watermark>5</low_watermark>
            <!--
               When the writer's cache is filled to this level, it
               will begin sending heartbeats at a faster rate in
               order to spur faster acknowledgement (positive or
               negative) of its samples to allow it to empty its
               cache and avoid blocking.
              -->
            <high_watermark>15</high_watermark>

            <!--
               If the number of samples in the writer's cache hasn't
               risen to high_watermark, this is the rate at which
               the DataWriter will send out periodic heartbeats.
              -->
            <heartbeat_period>
              <!-- 100 milliseconds: -->
              <sec>0</sec>
              <nanosec>100000000</nanosec>
            </heartbeat_period>
            <!--
               If the number of samples in the writer's cache has
               risen to high_watermark, and has not yet fallen to
               low_watermark, this is the rate at which the writer
               will send periodic heartbeats to its readers.
              -->
            <fast_heartbeat_period>
              <!-- 10 milliseconds: -->
              <sec>0</sec>
              <nanosec>10000000</nanosec>
            </fast_heartbeat_period>
            <!--
               If a durable reader starts up after the writer
               already has some samples in its cache, this is the
               rate at which it will heartbeat the new reader. It
               should generally be a shorter period of time than the
               normal heartbeat period in order to help the new
               reader catch up.
              -->
            <late_joiner_heartbeat_period>
              <!-- 10 milliseconds: -->
              <sec>0</sec>
              <nanosec>10000000</nanosec>
            </late_joiner_heartbeat_period>

            <!--
               The number of times a reliable writer will send a
               heartbeat to a reader without receiving a response
               before it will consider the reader to be inactive and
               no longer await acknowledgements before discarding
               sent data.

               On a non-real-time operating system like Windows or
               Linux, a poorly behaving process could monopolize the
               CPU for several seconds. Therefore, in many cases a
               value that yields a "grace period" of several seconds
               is a good choice.
              -->
            <max_heartbeat_retries>500</max_heartbeat_retries>

            <!--
               When a DataWriter receives a negative acknowledgement
               (NACK) from a DataReader for a particular data sample,
               it will send a repair packet to that reader.

               The amount of time the writer waits between receiving
               the NACK and sending the repair will be a random
               value in between the minimum and maximum values
               specified here. Narrowing the range, and shifting it
               towards zero, will make the writer more reactive.
               However, by leaving some delay, you increase the
               chances that the writer will learn of additional
               readers that missed the same data, in which case it
               will be able to send a single multicast repair
               instead of multiple unicast repairs, thereby using
               the available network bandwidth more efficiently. The
               higher the number of readers on the topic, and the
               greater the load on your network, the more you should
               consider specifying a range here.
              -->
            <min_nack_response_delay>
              <sec>0</sec>
	      <nanosec>0</nanosec>
            </min_nack_response_delay>
            <max_nack_response_delay>
              <sec>0</sec>
              <nanosec>0</nanosec>
            </max_nack_response_delay>
            <!--
               Set the maximum number of unacknowedged samples 
               (batches) in the DataWriter's queue equal to the max 
               number of batches, to limit how far ahead a writer can 
               get ahead of its potentially slow readers.
              -->
            <min_send_window_size>32</min_send_window_size>
            <max_send_window_size>32</max_send_window_size>
          </rtps_reliable_writer>
        </protocol>
      </datawriter_qos>

      <participant_qos>
        <!--
           The participant name, if it is set, will be displayed in the
           RTI Analyzer tool, making it easier for you to tell one
           application from another when you're debugging.
          -->
        <participant_name>
          <name>RTI Example (Reliable)</name>
        </participant_name>
      </participant_qos>
    </qos_profile>
  </qos_library>



  <qos_library name="experiments">
    <!-- QoS profile used to configure reliable communication between the DataWriter 
	 and DataReader created in the example code.

A QoS profile groups a set of related QoS.
    -->
    <qos_profile name="cft_profile">
      <!-- QoS used to configure the data writer created in the example code -->                
      <datawriter_qos>
	<reliability>
	  <kind>RELIABLE_RELIABILITY_QOS</kind>
	  <max_blocking_time>
	    <sec>60</sec>
	  </max_blocking_time>
	</reliability>                

	<history>
	  <kind>KEEP_ALL_HISTORY_QOS</kind>
	</history>

	<resource_limits>
	  <max_samples>50</max_samples>
	</resource_limits>

	<protocol>
	  <rtps_reliable_writer>
	    <min_send_window_size>50</min_send_window_size>
	    <max_send_window_size>50</max_send_window_size>
	  </rtps_reliable_writer>
	</protocol>
      </datawriter_qos>

      <!-- QoS used to configure the data reader created in the example code -->                
      <datareader_qos>
	<reliability>
	  <kind>RELIABLE_RELIABILITY_QOS</kind>
	</reliability>                

	<history>
	  <kind>KEEP_ALL_HISTORY_QOS</kind>
	</history>
      </datareader_qos>

      <participant_qos>
	<!--
	    The participant name, if it is set, will be displayed in the
	    RTI Analyzer tool, making it easier for you to tell one
	    application from another when you're debugging.
	-->
	<participant_name>
	  <name>RTI "Hello, World" Example</name>
	</participant_name>
        <discovery>
	  <initial_peers>
	    <element>builtin.udpv4://127.0.0.1</element>
	  </initial_peers>
	  <multicast_receive_addresses/>
	</discovery>
	<property>   
	  <value>
	    <element>
	      <name>dds.transport.UDPv4.builtin.multicast_enabled</name>
	      <value>0</value>
	    </element>
          </value>
        </property>
	
      </participant_qos>
    </qos_profile>

  </qos_library>

  <qos_library name="testing">
    <qos_profile name="test">
    <datawriter_qos>
      <reliability>
    	<kind>RELIABLE_RELIABILITY_QOS</kind>
    	<max_blocking_time>
    	  <sec>60</sec>
    	</max_blocking_time>
      </reliability>
      <deadline>
    	<period>
    	  <sec>9</sec>
	  <nanosec>0</nanosec>
    	</period>
      </deadline>
      <history>
    	<kind>KEEP_ALL_HISTORY_QOS</kind>
      </history>

      <resource_limits>
    	<max_samples>50</max_samples>
      </resource_limits>

      <protocol>
    	<rtps_reliable_writer>
    	  <min_send_window_size>50</min_send_window_size>
    	  <max_send_window_size>50</max_send_window_size>
    	</rtps_reliable_writer>
      </protocol>
    </datawriter_qos>
	</qos_profile>
  </qos_library>
</dds>
